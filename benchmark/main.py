import json
import logging
import time
import numpy as np
import torch
import os
import pathlib

new_cache_dir = pathlib.Path(os.path.join('backend', 'files', 'hf_cache'))
new_cache_dir.mkdir(parents=True, exist_ok=True) 
os.environ['HF_HOME'] = str(new_cache_dir)
os.environ['TRANSFORMERS_CACHE'] = str(new_cache_dir / "models")

from tqdm import tqdm
from docx import Document as DocxDocument
from sentence_transformers import CrossEncoder
from rank_bm25 import BM25Okapi
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain.schema import Document
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
VECTOR_STORE = None
EMBEDDINGS_MODEL = None
RERANKER = None

BM25_INDEX = None
BM25_CORPUS = []
FULL_INDEXED_DOCUMENTS = []


QA_CONFIG = {
    "QA_IS_ACTIVE": True,
    "QA_DOCX": pathlib.Path(os.path.join('backend', 'files', 'docs')),

    "K_VEC": 40,
    "K_BM25": 40,
    "ALPHA": 0.5,
    "TOP_N_RERANKER": 10,
    "CONFIDENCE_THRESHOLD": 0.55,

    "EMBEDDING_MODEL_NAME": "BAAI/bge-m3",

    "RERANKER_MODEL_NAME": "BAAI/bge-reranker-v2-m3",
    "RERANKER_MAX_LENGTH": 512,

    "QDRANT_NAME": "transconsultant_kb",
    "QDRANT_PATH": pathlib.Path(os.path.join('backend', 'files', 'qdrant_storage')),
    "QDRANT_BATCH_SIZE": 32,
}

HEADER_PRIORITY = {'H0': 6.0, 'H1': 5.0, 'H2': 4.0, 'H3': 3.0, 'H4': 2.0, 'T': 1.0, 'L': 1.0}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ QA
def initialize_embeddings():
    global EMBEDDINGS_MODEL

    embedding_model_name = QA_CONFIG.get("EMBEDDING_MODEL_NAME")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Embedding: {embedding_model_name}")
    try:
        EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs={'device': device}
        )
        logger.info(f"‚úÖ Embedding –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞: {device}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Embedding: {e}")
        EMBEDDINGS_MODEL = None

def initialize_reranker():
    global RERANKER

    reranker_model_name = QA_CONFIG.get("RERANKER_MODEL_NAME", "BAAI/bge-reranker-v2-m3")
    reranker_max_length = QA_CONFIG.get("RERANKER_MAX_LENGTH", 512)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Reranker: {reranker_model_name}")
    try:
        RERANKER = CrossEncoder(
            reranker_model_name,
            max_length=reranker_max_length,
            device=device
        )
        logger.info(f"‚úÖ Reranker –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞: {device}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Reranker: {e}")
        RERANKER = None

def initialize_vector_store():
    global VECTOR_STORE, EMBEDDINGS_MODEL
    logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant...")
    if not EMBEDDINGS_MODEL:
        logger.error("‚ùå EMBEDDINGS_MODEL –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return
    try:
        qdrant_path = QA_CONFIG.get("QDRANT_PATH")
        qdrant_name = QA_CONFIG.get("QDRANT_NAME")
        client = QdrantClient(path=qdrant_path)

        VECTOR_STORE = Qdrant(
            client=client, 
            collection_name=qdrant_name, 
            embeddings=EMBEDDINGS_MODEL
        )
        logger.info("‚úÖ Qdrant –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Qdrant: {e}")
        VECTOR_STORE = None

def initialize_qa_system():
    global EMBEDDINGS_MODEL, RERANKER, VECTOR_STORE
    
    if not (EMBEDDINGS_MODEL and RERANKER and VECTOR_STORE):
        logger.info(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã QA")
        if not EMBEDDINGS_MODEL: initialize_embeddings()
        if not RERANKER: initialize_reranker()
        if EMBEDDINGS_MODEL and not VECTOR_STORE: initialize_vector_store()
        logger.info("‚úÖ QA —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")

# –ü–∞—Ä—Å–∏–Ω–≥ DOCX
def is_bold_paragraph(p):
    text = p.text.strip()
    if not text:
        return False

    for run in p.runs:
        if run.text.strip(): 
            return run.bold or ('strong' in run.element.xml) 
    return False

def equals_indent(indent_emu, target_cm, eps_cm=0.01):
    return abs(indent_emu.cm - target_cm) < eps_cm

def get_chunk_type(paragraph):
    style_name = paragraph.style.name.lower()
    text = paragraph.text.strip()
    if not text: return None
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å—Ç–∏–ª–∏ (H0, H1, H2)
    if style_name.startswith('heading 1') or '–∑–∞–≥–æ–ª–æ–≤–æ–∫ 1' in style_name:
        return 'H0'
    if style_name.startswith('heading 2') or '–∑–∞–≥–æ–ª–æ–≤–æ–∫ 2' in style_name:
        return 'H1'
    if style_name.startswith('heading 3') or '–∑–∞–≥–æ–ª–æ–≤–æ–∫ 3' in style_name:
        return 'H2'
        
    # –õ–æ–≥–∏–∫–∞ –¥–ª—è H3 –∏ H4 ("–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç" + –ñ–∏—Ä–Ω–æ—Å—Ç—å + –û—Ç—Å—Ç—É–ø)
    is_normal_style = 'normal' in style_name or '–æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç' in style_name
    
    if is_normal_style and is_bold_paragraph(paragraph):

        first_line_indent_style = paragraph.style.paragraph_format.first_line_indent
        first_line_indent = paragraph.paragraph_format.first_line_indent

        if first_line_indent is not None and equals_indent(first_line_indent, 1.25):
            return 'H4'
        
        if first_line_indent is None and equals_indent(first_line_indent_style, 0.75):
            return 'H3'
        
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∏—Å–∫–∏ L
    if 'list' in style_name or 'bullet' in style_name or 'number' in style_name:
        return 'L'

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ü–∏—Ç–∞—Ç—ã, –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç T
    if 'quote' in style_name or 'normal' in style_name:
        return None
        
    return None

def parse_document_universal(file_path):
    logger.info(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {file_path.name}")
    
    if file_path.suffix.lstrip('.') != 'docx':
        logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª: {file_path.name}. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ DOCX.")
        return []
         
    documents = []
    paragraphs_data = [p for p in DocxDocument(file_path).paragraphs if p.text.strip()]
        
    i = 0
    paragraphs_data_count = len(paragraphs_data)
    
    current_header_h0: str = "" 
    current_header_h1: str = "" 
    current_header_h2: str = ""
    current_header_h3: str = "" 
    current_header_h4: str = ""
    
    while i < paragraphs_data_count:
        
        current_paragraph_obj = paragraphs_data[i]
        current_paragraph = current_paragraph_obj.text.strip()
        header_level = get_chunk_type(current_paragraph_obj)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –∞–±–∑–∞—Ü ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if header_level and header_level != 'L':
            
            # –õ–æ–≥–∏–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ H0-H4)
            if header_level == 'H0':
                current_header_h0 = current_paragraph
                current_header_h1 = current_header_h2 = current_header_h3 = current_header_h4 = ""
            elif header_level == 'H1':
                current_header_h1 = current_paragraph
                current_header_h2 = current_header_h3 = current_header_h4 = ""
            elif header_level == 'H2':
                current_header_h2 = current_paragraph
                current_header_h3 = current_header_h4 = ""
            elif header_level == 'H3':
                current_header_h3 = current_paragraph
                current_header_h4 = ""
            elif header_level == 'H4':
                current_header_h4 = current_paragraph
            
            header_context_parts = [h for h in [current_header_h0, current_header_h1, current_header_h2, current_header_h3, current_header_h4] if h]
            header_context = " | ".join(header_context_parts)
            header_chunk_content = f"[{header_context}] {current_paragraph}"
            
            documents.append(
                Document(
                    page_content=header_chunk_content,
                    metadata={
                        "source": file_path.name,
                        "chunk_type": header_level, 
                        "h0_header": current_header_h0,
                        "h1_header": current_header_h1,
                        "h2_header": current_header_h2,
                        "h3_header": current_header_h3,
                        "h4_header": current_header_h4,
                        "start_index": i,            
                        "end_index": i,              
                        "original_text": current_paragraph
                    }
                )
            )
            i += 1 
            continue
            
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ T –∏ L
        current_chunk_raw_texts = [current_paragraph]
        start_paragraph_index = i
        j = i
        final_chunk_type = header_level if header_level is not None else 'T'
        
        ends_with_colon = current_paragraph.rstrip().endswith(':')
        should_aggregate = False

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç ":" + –°–ø–∏—Å–æ–∫ "L"
        if final_chunk_type == 'T' and ends_with_colon and (j + 1 < paragraphs_data_count):
            next_paragraph_type = get_chunk_type(paragraphs_data[j + 1])
            if next_paragraph_type == 'L':
                should_aggregate = True
        
        # –¶–∏–∫–ª –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        if should_aggregate:
            final_chunk_type = 'T' 

            while j + 1 < paragraphs_data_count:
                next_paragraph_obj = paragraphs_data[j + 1]
                next_paragraph_text = next_paragraph_obj.text.strip()
                
                if not next_paragraph_text:
                    j += 1
                    continue

                next_header_level = get_chunk_type(next_paragraph_obj) 

                if next_header_level != 'L':
                    break
                    
                current_chunk_raw_texts.append(next_paragraph_text)
                j += 1
        
        current_chunk_raw = "\n".join(current_chunk_raw_texts)
        
        header_context_parts = [h for h in [current_header_h0, current_header_h1, current_header_h2, current_header_h3, current_header_h4] if h]
        header_context = " | ".join(header_context_parts)
        final_chunk_content = f"[{header_context}] {current_chunk_raw}"

        documents.append(
            Document(
                page_content=final_chunk_content,
                metadata={
                    "source": file_path.name,
                    "chunk_type": final_chunk_type, 
                    "h0_header": current_header_h0,
                    "h1_header": current_header_h1,
                    "h2_header": current_header_h2,
                    "h3_header": current_header_h3,
                    "h4_header": current_header_h4,
                    "start_index": start_paragraph_index,
                    "end_index": j,
                    "original_text": current_chunk_raw
                }
            )
        )
        
        # –ü–µ—Ä–µ–¥–≤–∏–≥–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω–¥–µ–∫—Å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –∞–±–∑–∞—Ü –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö
        i = j + 1 

    logger.info(f"üìö –†–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ {len(documents)} —á–∞–Ω–∫–æ–≤.")
    return documents


# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞
def update_bm25_index(documents):
    global BM25_INDEX, BM25_CORPUS, FULL_INDEXED_DOCUMENTS

    texts = [doc.page_content for doc in documents]

    if not FULL_INDEXED_DOCUMENTS or len(FULL_INDEXED_DOCUMENTS) == 0:
         BM25_CORPUS = []
         FULL_INDEXED_DOCUMENTS = []
    
    BM25_CORPUS.extend(texts)
    FULL_INDEXED_DOCUMENTS.extend(documents)
    tokenized_corpus = [doc.split(" ") for doc in BM25_CORPUS]
    BM25_INDEX = BM25Okapi(tokenized_corpus)
    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å—ã (Qdrant, BM25) –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (FULL_INDEXED_DOCUMENTS) –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")

def chunk_list(data, size):
    for i in range(0, len(data), size):
        yield data[i:i + size]

def index_document(file_path):
    global VECTOR_STORE

    documents = parse_document_universal(file_path)
    BATCH_SIZE = QA_CONFIG.get("QDRANT_BATCH_SIZE", 32)

    if not documents: return
    if VECTOR_STORE:
        try:
            logger.info(f"üîÑ Qdrant: –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {len(documents)} —á–∞–Ω–∫–æ–≤ (–±–∞—Ç—á: {BATCH_SIZE}).")
            chunk_batches = list(chunk_list(documents, BATCH_SIZE))
            
            for batch in tqdm(chunk_batches, desc=f"Qdrant: {file_path.name}", unit="batch"):
                VECTOR_STORE.add_documents(batch) 
            
            logger.info(f"‚úÖ Qdrant: –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤.")
        except Exception as e:
             logger.error(f"‚ùå –û—à–∏–±–∫–∞ Qdrant –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")
    update_bm25_index(documents)

def index_knowledge_base(folder_path):
    logger.info(f"üìö –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
    if not folder_path.exists():
        logger.error(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
        return
    
    supported_formats = ['.docx'] 
    files_to_index = []

    for ext in supported_formats:
        files_to_index.extend([f for f in folder_path.glob(f"*{ext}") if not f.name.startswith('~$')])
    
    if not files_to_index:
        logger.warning(f"‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ DOCX –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ {folder_path}")
        return
    
    if VECTOR_STORE:
        VECTOR_STORE.client.recreate_collection(
            collection_name=VECTOR_STORE.collection_name,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
        logger.info(f"üóëÔ∏è Qdrant: –ö–æ–ª–ª–µ–∫—Ü–∏—è '{VECTOR_STORE.collection_name}' –æ—á–∏—â–µ–Ω–∞.")
        
    global BM25_CORPUS, BM25_INDEX, FULL_INDEXED_DOCUMENTS
    BM25_CORPUS = []
    BM25_INDEX = None
    FULL_INDEXED_DOCUMENTS = []

    for file_path in tqdm(files_to_index, desc="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è"):
        index_document(file_path)

    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    logger.info(f"    –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {len(FULL_INDEXED_DOCUMENTS)}")

# –†–µ—Ç–∏–≤–µ—Ä
class CustomRetriever:
    def __init__(self, vector_store, reranker):
        self.vector_store = vector_store
        self.reranker = reranker

    def get_relevant_documents(self, query):
        global BM25_INDEX, BM25_CORPUS
        
        vec_results = self.vector_store.similarity_search_with_score(query, k=QA_CONFIG.get("K_VEC", 40))
        bm25_results = []

        if BM25_INDEX and BM25_CORPUS:
            tokenized_query = query.split(" ")
            doc_scores = BM25_INDEX.get_scores(tokenized_query)
            top_n_indices = np.argsort(doc_scores)[::-1][:QA_CONFIG.get("K_BM25", 40)]
            
            for idx in top_n_indices:
                doc = FULL_INDEXED_DOCUMENTS[idx]
                bm25_results.append(doc)

        all_docs_map = {}
        K_RRF = 60 
        def calculate_rrf_score(rank, k=K_RRF): return 1 / (k + rank)

        for rank, (doc, _) in enumerate(vec_results):
            text_hash = hash(doc.page_content)
            if text_hash not in all_docs_map: all_docs_map[text_hash] = {"doc": doc, "rrf_score": 0.0}
            all_docs_map[text_hash]["rrf_score"] += calculate_rrf_score(rank + 1) * QA_CONFIG.get("ALPHA", 0.5)

        for rank, doc in enumerate(bm25_results):
            text_hash = hash(doc.page_content)
            if text_hash not in all_docs_map: all_docs_map[text_hash] = {"doc": doc, "rrf_score": 0.0}
            all_docs_map[text_hash]["rrf_score"] += calculate_rrf_score(rank + 1) * (1.0 - QA_CONFIG.get("ALPHA", 0.5))

        final_candidates = sorted(all_docs_map.values(), key=lambda x: x["rrf_score"], reverse=True)
        rerank_input = [item["doc"] for item in final_candidates[:50]]
        
        if self.reranker and len(rerank_input) > 0:
            pairs = [[query, doc.page_content] for doc in rerank_input]
            scores = self.reranker.predict(pairs)
            probabilities = 1 / (1 + np.exp(-scores))
            
            scored_documents = sorted(
                zip(rerank_input, probabilities),
                key=lambda x: x[1],
                reverse=True
            )
            return scored_documents[:QA_CONFIG.get("TOP_N_RERANKER", 5)]
        
        return []

# –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ—Ç–≤–µ—Ç
def expand_context(best_match_doc):
    global FULL_INDEXED_DOCUMENTS, HEADER_PRIORITY
    
    chunk_type = best_match_doc.metadata.get('chunk_type')
    original_text_target = best_match_doc.metadata.get('original_text', best_match_doc.page_content)
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    if chunk_type == 'T':
        return original_text_target
        
    target_level = chunk_type 
    target_priority = HEADER_PRIORITY.get(target_level, 0)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    start_index_in_full = best_match_doc.metadata.get('start_index', -1) 
    
    if start_index_in_full == -1:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: –ù–µ –Ω–∞–π–¥–µ–Ω start_index –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞. –í–æ–∑–≤—Ä–∞—Ç —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞.")
        return original_text_target
        
    expanded_text = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    expanded_text.append(f"<{target_level}> {original_text_target} </{target_level}>")
    
    # –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
    for idx in range(start_index_in_full + 1, len(FULL_INDEXED_DOCUMENTS)):
        current_doc = FULL_INDEXED_DOCUMENTS[idx]
        current_type = current_doc.metadata.get('chunk_type')
        current_priority = HEADER_PRIORITY.get(current_type, 0)
        current_original_text = current_doc.metadata.get('original_text', '')
        
        # –£—Å–ª–æ–≤–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è
        if current_type != 'T' and current_priority >= target_priority:
            break
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç (–¢–µ–∫—Å—Ç –∏–ª–∏ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–æ–ª–µ–µ –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è)
        if current_type == 'T':
            expanded_text.append(current_original_text)
        else:
            expanded_text.append(f"<{current_type}> {current_original_text} </{current_type}>")
            
    logger.info(f"‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–æ–±—Ä–∞–Ω–æ {len(expanded_text)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤.")
    return "\n\n".join(expanded_text)

def answer_question(question):
    global VECTOR_STORE, RERANKER, FULL_INDEXED_DOCUMENTS, HEADER_PRIORITY
    
    if not (VECTOR_STORE and RERANKER and FULL_INDEXED_DOCUMENTS):
        logger.error("‚ùå –°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞/–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        return {"answer": "–°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.", "source_documents": [], "confidence": 0.0, "latency_sec": 0.0}

    start_time = time.time()
    
    retriever = CustomRetriever(vector_store=VECTOR_STORE, reranker=RERANKER)
    scored_documents = retriever.get_relevant_documents(question)
    
    latency = time.time() - start_time
    
    if not scored_documents:
        return {"answer": "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.", "source_documents": [], "confidence": 0.0, "latency_sec": latency}

    best_match_doc = scored_documents[0][0]
    top_score = scored_documents[0][1]

    normalized_query = question.strip().lower()
    priority_match = None
    max_priority = -1
    
    # –õ–æ–≥–∏–∫–∞ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
    for doc, score in scored_documents[:QA_CONFIG.get("TOP_N_RERANKER", 5)]:
        doc_type = doc.metadata.get('chunk_type')
        doc_original_text = doc.metadata.get('original_text', '')
        
        if doc_type != 'T' and doc_original_text.strip().lower() == normalized_query and score > 0.8:
            priority = HEADER_PRIORITY.get(doc_type, 0)
            
            if priority > max_priority:
                max_priority = priority
                priority_match = doc

    if priority_match:
        best_match_doc = priority_match
        top_score = scored_documents[0][1] 
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    final_expanded_text = expand_context(best_match_doc)
    
    
    answer_text = f"{final_expanded_text}"
    answer_type = best_match_doc.metadata.get('chunk_type')
    
    logger.info(f"‚è±Ô∏è –û—Ç–≤–µ—Ç –¥–∞–Ω –∑–∞: {latency:.2f}s")

    if top_score < QA_CONFIG.get("CONFIDENCE_THRESHOLD", 0.55):
        return {
            "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.",
            "source_documents": [best_match_doc],
            "confidence": top_score,
            "latency_sec": latency,
            "chunk_type": answer_type
        }

    return {
        "answer": answer_text,
        "source_documents": [best_match_doc],
        "confidence": top_score,
        "latency_sec": latency,
        "chunk_type": answer_type
    }

def create_benchmark_from_qdrant(output_file="benchmark_dataset.json"):
    global VECTOR_STORE, FULL_INDEXED_DOCUMENTS
    
    if not VECTOR_STORE:
        logger.error("‚ùå VECTOR_STORE –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return
    
    logger.info("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    
    benchmark_data = []
    
    for idx, doc in enumerate(FULL_INDEXED_DOCUMENTS):
        entry = {
            "id": idx,
            "context": doc.page_content,
            "chunk_type": doc.metadata.get('chunk_type'),
            "source": doc.metadata.get('source'),
            "h0_header": doc.metadata.get('h0_header', ''),
            "h1_header": doc.metadata.get('h1_header', ''),
            "h2_header": doc.metadata.get('h2_header', ''),
            "h3_header": doc.metadata.get('h3_header', ''),
            "h4_header": doc.metadata.get('h4_header', ''),
            "original_text": doc.metadata.get('original_text', ''),
            "start_index": doc.metadata.get('start_index'),
            "end_index": doc.metadata.get('end_index'),
            
            # –ü–æ–ª—è –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Ä—É—á–Ω—É—é/–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            "question": "",  # –ó–∞–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤—Ä—É—á–Ω—É—é
            "reference_answer": "",  # –≠—Ç–∞–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–≤—Ä—É—á–Ω—É—é)
            "generated_answer": "",  # –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            "top_k_contexts": [],  # Top-K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            "retrieval_scores": [],  # –°–∫–æ—Ä—ã —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            "confidence": 0.0,  # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            "latency_sec": 0.0  # –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        }
        
        benchmark_data.append(entry)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
    output_path = pathlib.Path(output_file)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(benchmark_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {output_path}")
    logger.info(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(benchmark_data)}")
    return output_path


def process_benchmark_questions(input_file="benchmark_dataset.json", 
                                output_file="benchmark_results.json",
                                top_k=10):
    global VECTOR_STORE, RERANKER, FULL_INDEXED_DOCUMENTS
    
    if not (VECTOR_STORE and RERANKER and FULL_INDEXED_DOCUMENTS):
        logger.error("‚ùå –°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        return
    
    logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞: {input_file}")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    input_path = pathlib.Path(input_file)
    if not input_path.exists():
        logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {input_file}")
        return
    
    with open(input_path, 'r', encoding='utf-8') as f:
        benchmark_data = json.load(f)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
    processed_count = 0
    skipped_count = 0
    
    for entry in benchmark_data:
        question = entry.get("question", "").strip()
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        if not question:
            skipped_count += 1
            continue
        
        logger.info(f"‚ùì –í–æ–ø—Ä–æ—Å {entry['id']}: {question[:50]}...")
        
        try:
            start_time = time.time()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ —Ä–µ—Ç—Ä–∏–≤–µ—Ä
            retriever = CustomRetriever(vector_store=VECTOR_STORE, reranker=RERANKER)
            scored_documents = retriever.get_relevant_documents(question)
            
            latency = time.time() - start_time
            
            if not scored_documents:
                entry["generated_answer"] = "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."
                entry["confidence"] = 0.0
                entry["latency_sec"] = latency
                entry["top_k_contexts"] = []
                entry["retrieval_scores"] = []
                processed_count += 1
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º Top-K –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
            top_k_docs = scored_documents[:min(top_k, len(scored_documents))]
            
            entry["top_k_contexts"] = [
                {
                    "rank": i + 1,
                    "context": doc.page_content,
                    "chunk_type": doc.metadata.get('chunk_type'),
                    "source": doc.metadata.get('source'),
                    "original_text": doc.metadata.get('original_text', ''),
                    "score": float(score)
                }
                for i, (doc, score) in enumerate(top_k_docs)
            ]
            
            entry["retrieval_scores"] = [float(score) for _, score in top_k_docs]
            
            # –ü–æ–ª—É—á–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            best_match_doc = scored_documents[0][0]
            top_score = scored_documents[0][1]
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª–æ–≥–∏–∫—É –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
            normalized_query = question.strip().lower()
            priority_match = None
            max_priority = -1
            
            for doc, score in scored_documents[:QA_CONFIG.get("TOP_N_RERANKER", 5)]:
                doc_type = doc.metadata.get('chunk_type')
                doc_original_text = doc.metadata.get('original_text', '')
                
                if doc_type != 'T' and doc_original_text.strip().lower() == normalized_query and score > 0.8:
                    priority = HEADER_PRIORITY.get(doc_type, 0)
                    
                    if priority > max_priority:
                        max_priority = priority
                        priority_match = doc
            
            if priority_match:
                best_match_doc = priority_match
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            final_expanded_text = expand_context(best_match_doc)
            
            entry["generated_answer"] = final_expanded_text
            entry["confidence"] = float(top_score)
            entry["latency_sec"] = latency
            
            processed_count += 1
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ. Confidence: {top_score:.3f}, Latency: {latency:.2f}s")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞ {entry['id']}: {e}")
            entry["generated_answer"] = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
            entry["confidence"] = 0.0
            continue
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_path = pathlib.Path(output_file)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(benchmark_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count}, –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")
    logger.info(f"   –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(benchmark_data)}")
    
    return output_path


def calculate_retrieval_metrics(benchmark_file="benchmark_results.json"):
    logger.info(f"üìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ –∏–∑: {benchmark_file}")
    
    with open(benchmark_file, 'r', encoding='utf-8') as f:
        benchmark_data = json.load(f)
    
    ndcg_scores = []
    mrr_scores = []
    ap_scores = []
    
    for entry in benchmark_data:
        if not entry.get("question") or not entry.get("top_k_contexts"):
            continue
        
        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç - —ç—Ç–æ —Ç–æ—Ç, –∏–∑ –∫–æ—Ç–æ—Ä–æ–≥–æ –≤–∑—è—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç
        relevant_context = entry.get("context", "")
        top_k_contexts = entry.get("top_k_contexts", [])
        
        if not top_k_contexts:
            continue
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        relevant_position = None
        for i, ctx in enumerate(top_k_contexts[:100]):  # MAP@100
            if ctx.get("context") == relevant_context:
                relevant_position = i + 1
                break
        
        # NDCG@10
        if relevant_position and relevant_position <= 10:
            dcg = 1.0 / np.log2(relevant_position + 1)
            idcg = 1.0 / np.log2(2)
            ndcg_scores.append(dcg / idcg)
        else:
            ndcg_scores.append(0.0)
        
        # MRR@10
        if relevant_position and relevant_position <= 10:
            mrr_scores.append(1.0 / relevant_position)
        else:
            mrr_scores.append(0.0)
        
        # MAP@100
        if relevant_position and relevant_position <= 100:
            ap_scores.append(1.0 / relevant_position)
        else:
            ap_scores.append(0.0)
    
    metrics = {
        "ndcg@10": float(np.mean(ndcg_scores)) if ndcg_scores else 0.0,
        "mrr@10": float(np.mean(mrr_scores)) if mrr_scores else 0.0,
        "map@100": float(np.mean(ap_scores)) if ap_scores else 0.0,
        "num_queries": len(ndcg_scores)
    }
    
    logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞:")
    logger.info(f"   NDCG@10: {metrics['ndcg@10']:.4f}")
    logger.info(f"   MRR@10: {metrics['mrr@10']:.4f}")
    logger.info(f"   MAP@100: {metrics['map@100']:.4f}")
    logger.info(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {metrics['num_queries']}")
    
    return metrics


if __name__ == "__main__":
    logger.info("‚åõ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è QA —Å–∏—Å—Ç–µ–º—ã")
    logger.info("–®–∞–≥ 1/2: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤")
    initialize_qa_system()
    
    if VECTOR_STORE:
        logger.info("–®–∞–≥ 2/2: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
        index_knowledge_base(pathlib.Path(QA_CONFIG.get("QA_DOCX")))
    
    logger.info("‚úÖ QA —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ.")
    
    while True:
        # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã
        print("\n–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:")
        print("1. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º (–≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç)")
        print("2. –°–æ–∑–¥–∞—Ç—å –±–µ–Ω—á–º–∞—Ä–∫ –¥–∞—Ç–∞—Å–µ—Ç (–í–ù–ò–ú–ê–ù–ò–ï! –ü–µ—Ä–µ–∑–∞–ø–∏—à–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ–∞–π–ª, –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–∏—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –≤—Ä—É—á–Ω—É—é)")
        print("3. –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–Ω—á–º–∞—Ä–∫ –¥–∞—Ç–∞—Å–µ—Ç")
        print("4. –í—ã—á–∏—Å–ª–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞")
        print("0. –í—ã—Ö–æ–¥")
        
        mode = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä —Ä–µ–∂–∏–º–∞ (1-4): ").strip()
        
        if mode == "1":
            # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
            while True:
                user_question = input("\n–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
                if user_question.lower() == 'exit':
                    break
                
                response = answer_question(user_question)
                
                print(f"\nüìù –û—Ç–≤–µ—Ç:\n{response['answer']}")
                print(f"\nüìä –î–æ–≤–µ—Ä–∏–µ: {response['confidence']:.2f}, –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {response['latency_sec']:.2f}s")
                if response['source_documents']:
                    print(f"üìÇ –ò—Å—Ç–æ—á–Ω–∏–∫: {response['source_documents'][0].metadata.get('source')} (–¢–∏–ø —á–∞–Ω–∫–∞: {response.get('chunk_type', 'N/A')})")
                else:
                    print("üìÇ –ò—Å—Ç–æ—á–Ω–∏–∫: –ù–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        elif mode == "2":
            # –°–æ–∑–¥–∞–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∞
            output_file = os.path.join("benchmark", "benchmark_dataset.json")
            create_benchmark_from_qdrant(output_file)
            logger.info(f"‚úÖ –¢–µ–ø–µ—Ä—å –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –ø–æ–ª—è 'question' –∏ 'reference_answer' –≤ —Ñ–∞–π–ª–µ {output_file}")
        
        elif mode == "3":
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞
            input_file = os.path.join("benchmark", "benchmark_dataset.json")
            output_file = os.path.join("benchmark", "benchmark_results.json")
            top_k = QA_CONFIG.get("TOP_N_RERANKER", 5)
            
            process_benchmark_questions(input_file, output_file, top_k)
        
        elif mode == "4":
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            benchmark_file = os.path.join("benchmark", "benchmark_results.json")
            
            calculate_retrieval_metrics(benchmark_file)
        
        elif mode == "0":
            logger.info("üëã –í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã.")
            break

        else:
            logger.warning("‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ä–µ–∂–∏–º. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ–≥—Ä–∞–º–º—É —Å–Ω–æ–≤–∞."