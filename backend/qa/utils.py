import os
import logging
from pathlib import Path

new_cache_dir = Path("D:/Programs/HuggingFace_Cache") 
new_cache_dir.mkdir(parents=True, exist_ok=True) 
os.environ['HF_HOME'] = str(new_cache_dir)
os.environ['TRANSFORMERS_CACHE'] = str(new_cache_dir / "models")

from typing import List, Dict, Any, Optional, Tuple
import torch
import time
import numpy as np
import warnings
import sys
from tqdm import tqdm

from sentence_transformers import CrossEncoder
from rank_bm25 import BM25Okapi

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain.schema import Document
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

from docx import Document as DocxDocument
from docx.text.paragraph import Paragraph


logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("QA")
logger.setLevel(logging.INFO)
warnings.filterwarnings("ignore", category=UserWarning)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
VECTOR_STORE: Optional[Qdrant] = None
EMBEDDINGS_MODEL: Optional[HuggingFaceEmbeddings] = None
RERANKER: Optional[CrossEncoder] = None

BM25_INDEX: Optional[BM25Okapi] = None
BM25_CORPUS: List[str] = []
FULL_INDEXED_DOCUMENTS: List[Document] = [] 

HEADER_PRIORITY = {'H0': 6.0, 'H1': 5.0, 'H2': 4.0, 'H3': 3.0, 'H4': 2.0, 'T': 1.0, 'L': 1.0}

CONFIG = {
    "K_VEC": 40,
    "K_BM25": 40,
    "ALPHA": 0.5,
    "TOP_N_RERANKER": 10,
    "CONFIDENCE_THRESHOLD": 0.55,
    "EMBEDDING_MODEL_NAME": "BAAI/bge-m3",
    "RERANKER_MODEL_NAME": "BAAI/bge-reranker-v2-m3",
}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ QA
def _initialize_embeddings():
    global EMBEDDINGS_MODEL
    logger.info(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Embedding: {CONFIG['EMBEDDING_MODEL_NAME']}")
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
            model_name=CONFIG["EMBEDDING_MODEL_NAME"],
            model_kwargs={'device': device}
        )
        logger.info(f"‚úÖ Embedding –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞: {device}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Embedding: {e}")
        EMBEDDINGS_MODEL = None

def _initialize_reranker():
    global RERANKER
    logger.info(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Reranker: {CONFIG['RERANKER_MODEL_NAME']}")
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        RERANKER = CrossEncoder(
            CONFIG["RERANKER_MODEL_NAME"], 
            max_length=512,
            device=device
        )
        logger.info(f"‚úÖ Reranker –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞: {device}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Reranker: {e}")
        RERANKER = None

def _initialize_vector_store():
    global VECTOR_STORE, EMBEDDINGS_MODEL
    logger.info("‚è≥ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant...")
    if not EMBEDDINGS_MODEL:
        logger.error("‚ùå EMBEDDINGS_MODEL –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return
    try:
        client = QdrantClient(path="./qdrant_storage") 
        collection_name = "transconsultant_kb"
        VECTOR_STORE = Qdrant(
            client=client, 
            collection_name=collection_name, 
            embeddings=EMBEDDINGS_MODEL
        )
        logger.info("‚úÖ Qdrant –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Qdrant: {e}")
        VECTOR_STORE = None

def initialize_qa_system(force_reload: bool = False):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã QA."""
    global EMBEDDINGS_MODEL, RERANKER, VECTOR_STORE
    
    if force_reload or not (EMBEDDINGS_MODEL and RERANKER and VECTOR_STORE):
        logger.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã QA")
        if not EMBEDDINGS_MODEL: _initialize_embeddings()
        if not RERANKER: _initialize_reranker()
        if EMBEDDINGS_MODEL and not VECTOR_STORE: _initialize_vector_store()
        logger.info("‚úÖ QA —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")

# –ü–∞—Ä—Å–∏–Ω–≥ DOCX
def is_bold_paragraph(p: Paragraph) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ (–∏–ª–∏ –µ–≥–æ –Ω–∞—á–∞–ª–æ) –∂–∏—Ä–Ω—ã–π."""
    text = p.text.strip()
    if not text:
        return False

    for run in p.runs:
        if run.text.strip(): 
            return run.bold or ('strong' in run.element.xml) 
    return False

def equals_indent(indent_emu, target_cm, eps_cm=0.01):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –æ—Ç—Å—Ç—É–ø –≤ EMU –ø—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–≤–µ–Ω target_cm —Å –¥–æ–ø—É—Å–∫–æ–º eps_cm."""
    return abs(indent_emu.cm - target_cm) < eps_cm

def _get_chunk_type(paragraph: Paragraph) -> Optional[str]:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –∑–∞–≥–æ–ª–æ–≤–∫–∞, —Å–ø–∏—Å–∫–∞ –∏–ª–∏ –æ–±—ã—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ ('T' —á–µ—Ä–µ–∑ None)."""
    style_name = paragraph.style.name.lower()
    text = paragraph.text.strip()
    if not text: return None
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å—Ç–∏–ª–∏ (H0, H1, H2)
    if style_name.startswith('heading 1') or '–∑–∞–≥–æ–ª–æ–≤–æ–∫ 1' in style_name:
        return 'H0'
    if style_name.startswith('heading 2') or '–∑–∞–≥–æ–ª–æ–≤–æ–∫ 2' in style_name:
        return 'H1'
    if style_name.startswith('heading 3') or '–∑–∞–≥–æ–ª–æ–≤–æ–∫ 3' in style_name:
        return 'H2'
        
    # –õ–æ–≥–∏–∫–∞ –¥–ª—è H3 –∏ H4 ("–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç" + –ñ–∏—Ä–Ω–æ—Å—Ç—å + –û—Ç—Å—Ç—É–ø)
    is_normal_style = 'normal' in style_name or '–æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç' in style_name
    
    if is_normal_style and is_bold_paragraph(paragraph):

        first_line_indent_style = paragraph.style.paragraph_format.first_line_indent
        first_line_indent = paragraph.paragraph_format.first_line_indent

        if first_line_indent is not None and equals_indent(first_line_indent, 1.25):
            return 'H4'
        
        if first_line_indent is None and equals_indent(first_line_indent_style, 0.75):
            return 'H3'
        
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∏—Å–∫–∏ L
    if 'list' in style_name or 'bullet' in style_name or 'number' in style_name:
        return 'L'

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ü–∏—Ç–∞—Ç—ã, –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç T
    if 'quote' in style_name or 'normal' in style_name:
        return None
        
    return None

def _parse_document_universal(file_path: Path) -> List[Document]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —É—Å–ª–æ–≤–Ω—ã–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º –∞–±–∑–∞—Ü–µ–≤: 
    - –¢–µ–∫—Å—Ç (T) + –°–ø–∏—Å–æ–∫ (L) –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è, –µ—Å–ª–∏ T –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –¥–≤–æ–µ—Ç–æ—á–∏–µ–º –∏ –∑–∞ –Ω–∏–º –∏–¥–µ—Ç L.
    """
    logger.info(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {file_path.name}")
    
    if file_path.suffix.lstrip('.') != 'docx':
        logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª: {file_path.name}. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ DOCX.")
        return []
         
    documents: List[Document] = []
    paragraphs_data = [p for p in DocxDocument(file_path).paragraphs if p.text.strip()]
        
    i = 0
    paragraphs_data_count = len(paragraphs_data)
    
    current_header_h0: str = "" 
    current_header_h1: str = "" 
    current_header_h2: str = ""
    current_header_h3: str = ""
    current_header_h4: str = ""
    
    while i < paragraphs_data_count:
        
        current_paragraph_obj = paragraphs_data[i]
        current_paragraph = current_paragraph_obj.text.strip()
        header_level = _get_chunk_type(current_paragraph_obj)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –∞–±–∑–∞—Ü ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if header_level and header_level != 'L':
            
            # –õ–æ–≥–∏–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ H0-H4)
            if header_level == 'H0':
                current_header_h0 = current_paragraph
                current_header_h1 = current_header_h2 = current_header_h3 = current_header_h4 = ""
            elif header_level == 'H1':
                current_header_h1 = current_paragraph
                current_header_h2 = current_header_h3 = current_header_h4 = ""
            elif header_level == 'H2':
                current_header_h2 = current_paragraph
                current_header_h3 = current_header_h4 = ""
            elif header_level == 'H3':
                current_header_h3 = current_paragraph
                current_header_h4 = ""
            elif header_level == 'H4':
                current_header_h4 = current_paragraph
            
            header_context_parts = [h for h in [current_header_h0, current_header_h1, current_header_h2, current_header_h3, current_header_h4] if h]
            header_context = " | ".join(header_context_parts)
            header_chunk_content = f"[{header_context}] {current_paragraph}"
            
            documents.append(
                Document(
                    page_content=header_chunk_content,
                    metadata={
                        "source": file_path.name,
                        "chunk_type": header_level, 
                        "h0_header": current_header_h0,
                        "h1_header": current_header_h1,
                        "h2_header": current_header_h2,
                        "h3_header": current_header_h3,
                        "h4_header": current_header_h4,
                        "start_index": i,            
                        "end_index": i,              
                        "original_text": current_paragraph
                    }
                )
            )
            i += 1 
            continue
            
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ T –∏ L
        current_chunk_raw_texts = [current_paragraph]
        start_paragraph_index = i
        j = i
        final_chunk_type = header_level if header_level is not None else 'T'
        
        ends_with_colon = current_paragraph.rstrip().endswith(':')
        should_aggregate = False

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç ":" + –°–ø–∏—Å–æ–∫ "L"
        if final_chunk_type == 'T' and ends_with_colon and (j + 1 < paragraphs_data_count):
            next_paragraph_type = _get_chunk_type(paragraphs_data[j + 1])
            if next_paragraph_type == 'L':
                should_aggregate = True
        
        # –¶–∏–∫–ª –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        if should_aggregate:
            final_chunk_type = 'T' 

            while j + 1 < paragraphs_data_count:
                next_paragraph_obj = paragraphs_data[j + 1]
                next_paragraph_text = next_paragraph_obj.text.strip()
                
                if not next_paragraph_text:
                    j += 1
                    continue

                next_header_level = _get_chunk_type(next_paragraph_obj) 

                if next_header_level != 'L':
                    break
                    
                current_chunk_raw_texts.append(next_paragraph_text)
                j += 1
        
        current_chunk_raw = "\n".join(current_chunk_raw_texts)
        
        header_context_parts = [h for h in [current_header_h0, current_header_h1, current_header_h2, current_header_h3, current_header_h4] if h]
        header_context = " | ".join(header_context_parts)
        final_chunk_content = f"[{header_context}] {current_chunk_raw}"

        documents.append(
            Document(
                page_content=final_chunk_content,
                metadata={
                    "source": file_path.name,
                    "chunk_type": final_chunk_type, 
                    "h0_header": current_header_h0,
                    "h1_header": current_header_h1,
                    "h2_header": current_header_h2,
                    "h3_header": current_header_h3,
                    "h4_header": current_header_h4,
                    "start_index": start_paragraph_index,
                    "end_index": j,
                    "original_text": current_chunk_raw
                }
            )
        )
        
        # –ü–µ—Ä–µ–¥–≤–∏–≥–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω–¥–µ–∫—Å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –∞–±–∑–∞—Ü –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö
        i = j + 1 

    logger.info(f"üìö –†–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ {len(documents)} —á–∞–Ω–∫–æ–≤ (H0, H1, H2, H3, H4, L, T).")
    return documents


# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞
def update_bm25_index(documents: List[Document]):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç BM25 –∏–Ω–¥–µ–∫—Å –∏ –∫–æ—Ä–ø—É—Å, –∞ —Ç–∞–∫–∂–µ –≥–ª–æ–±–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É."""
    global BM25_INDEX, BM25_CORPUS, FULL_INDEXED_DOCUMENTS
    texts = [doc.page_content for doc in documents]
    if not FULL_INDEXED_DOCUMENTS or len(FULL_INDEXED_DOCUMENTS) == 0:
         BM25_CORPUS = []
         FULL_INDEXED_DOCUMENTS = []
    BM25_CORPUS.extend(texts)
    FULL_INDEXED_DOCUMENTS.extend(documents)
    tokenized_corpus = [doc.split(" ") for doc in BM25_CORPUS]
    BM25_INDEX = BM25Okapi(tokenized_corpus)
    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å—ã (Qdrant, BM25) –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (FULL_INDEXED_DOCUMENTS) –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")

def index_document(file_path: Path):
    """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç: –ø–∞—Ä—Å–∏–Ω–≥, Qdrant –∏ BM25."""
    global VECTOR_STORE
    documents = _parse_document_universal(file_path) 
    if not documents: return
    if VECTOR_STORE:
        try:
            VECTOR_STORE.add_documents(documents)
            logger.info(f"‚úÖ Qdrant: –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤.")
        except Exception as e:
             logger.error(f"‚ùå –û—à–∏–±–∫–∞ Qdrant: {e}")
    update_bm25_index(documents)

def index_knowledge_base(folder_path: Path):
    """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (DOCX) –∏–∑ –ø–∞–ø–∫–∏ —Å –æ—á–∏—Å—Ç–∫–æ–π."""
    logger.info(f"üìö –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
    if not folder_path.exists():
        logger.error(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
        return
    
    supported_formats = ['.docx'] 
    files_to_index = []
    for ext in supported_formats:
        files_to_index.extend([f for f in folder_path.glob(f"*{ext}") if not f.name.startswith('~$')])
    
    if not files_to_index:
        logger.warning(f"‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ DOCX –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ {folder_path}")
        return
    
    if VECTOR_STORE:
        VECTOR_STORE.client.recreate_collection(
            collection_name=VECTOR_STORE.collection_name,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
        logger.info(f"üóëÔ∏è Qdrant: –ö–æ–ª–ª–µ–∫—Ü–∏—è '{VECTOR_STORE.collection_name}' –æ—á–∏—â–µ–Ω–∞.")
        
    global BM25_CORPUS, BM25_INDEX, FULL_INDEXED_DOCUMENTS
    BM25_CORPUS = []
    BM25_INDEX = None
    FULL_INDEXED_DOCUMENTS = []

    for file_path in tqdm(files_to_index, desc="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è"):
        index_document(file_path)

    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    logger.info(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {len(FULL_INDEXED_DOCUMENTS)}")

# –†–µ—Ç–∏–≤–µ—Ä
class CustomRetriever:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä (Vector + BM25) —Å Reranking."""
    vector_store: Qdrant
    reranker: CrossEncoder

    def __init__(self, vector_store, reranker):
        self.vector_store = vector_store
        self.reranker = reranker

    def get_relevant_documents(self, query: str) -> List[Tuple[Document, float]]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –∏ Reranking, –≤–æ–∑–≤—Ä–∞—â–∞—è –≤—Å–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."""
        global BM25_INDEX, BM25_CORPUS
        
        vec_results: List[Tuple[Document, float]] = self.vector_store.similarity_search_with_score(query, k=CONFIG["K_VEC"])
        bm25_results = []
        if BM25_INDEX and BM25_CORPUS:
            tokenized_query = query.split(" ")
            doc_scores = BM25_INDEX.get_scores(tokenized_query)
            top_n_indices = np.argsort(doc_scores)[::-1][:CONFIG["K_BM25"]]
            
            for idx in top_n_indices:
                doc = FULL_INDEXED_DOCUMENTS[idx]
                bm25_results.append(doc)

        all_docs_map: Dict[str, Dict[str, Any]] = {}
        K_RRF = 60 
        def calculate_rrf_score(rank, k=K_RRF): return 1 / (k + rank)

        for rank, (doc, _) in enumerate(vec_results):
            text_hash = hash(doc.page_content)
            if text_hash not in all_docs_map: all_docs_map[text_hash] = {"doc": doc, "rrf_score": 0.0}
            all_docs_map[text_hash]["rrf_score"] += calculate_rrf_score(rank + 1) * CONFIG["ALPHA"]

        for rank, doc in enumerate(bm25_results):
            text_hash = hash(doc.page_content)
            if text_hash not in all_docs_map: all_docs_map[text_hash] = {"doc": doc, "rrf_score": 0.0}
            all_docs_map[text_hash]["rrf_score"] += calculate_rrf_score(rank + 1) * (1.0 - CONFIG["ALPHA"])

        final_candidates = sorted(all_docs_map.values(), key=lambda x: x["rrf_score"], reverse=True)
        rerank_input = [item["doc"] for item in final_candidates[:50]]
        
        if self.reranker and len(rerank_input) > 0:
            pairs = [[query, doc.page_content] for doc in rerank_input]
            scores = self.reranker.predict(pairs)
            probabilities = 1 / (1 + np.exp(-scores))
            
            scored_documents: List[Tuple[Document, float]] = sorted(
                zip(rerank_input, probabilities),
                key=lambda x: x[1],
                reverse=True
            )
            return scored_documents[:CONFIG["TOP_N_RERANKER"]]
        
        return []

# –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ—Ç–≤–µ—Ç
def _expand_context(best_match_doc: Document) -> str:
    """ 
    –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ start_index –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –ø—Ä–∏–≤—è–∑–∫–∏.
    """
    global FULL_INDEXED_DOCUMENTS, HEADER_PRIORITY
    
    chunk_type = best_match_doc.metadata.get('chunk_type')
    original_text_target = best_match_doc.metadata.get('original_text', best_match_doc.page_content)
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    if chunk_type == 'T':
        return original_text_target
        
    target_level = chunk_type 
    target_priority = HEADER_PRIORITY.get(target_level, 0)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    start_index_in_full = best_match_doc.metadata.get('start_index', -1) 
    
    if start_index_in_full == -1:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: –ù–µ –Ω–∞–π–¥–µ–Ω start_index –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞. –í–æ–∑–≤—Ä–∞—Ç —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞.")
        return original_text_target
        
    expanded_text = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    expanded_text.append(f"<{target_level}> {original_text_target} </{target_level}>")
    
    # –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
    for idx in range(start_index_in_full + 1, len(FULL_INDEXED_DOCUMENTS)):
        current_doc = FULL_INDEXED_DOCUMENTS[idx]
        current_type = current_doc.metadata.get('chunk_type')
        current_priority = HEADER_PRIORITY.get(current_type, 0)
        current_original_text = current_doc.metadata.get('original_text', '')
        
        # –£—Å–ª–æ–≤–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è
        if current_type != 'T' and current_priority >= target_priority:
            break
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç (–¢–µ–∫—Å—Ç –∏–ª–∏ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–æ–ª–µ–µ –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è)
        if current_type == 'T':
            expanded_text.append(current_original_text)
        else:
            expanded_text.append(f"<{current_type}> {current_original_text} </{current_type}>")
            
    logger.info(f"‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–æ–±—Ä–∞–Ω–æ {len(expanded_text)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤.")
    return "\n\n".join(expanded_text)

def answer_question(question: str) -> Dict[str, Any]:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –∏—â–µ—Ç, –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, —Ä–∞—Å—à–∏—Ä—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç."""
    global VECTOR_STORE, RERANKER, FULL_INDEXED_DOCUMENTS, HEADER_PRIORITY
    
    if not (VECTOR_STORE and RERANKER and FULL_INDEXED_DOCUMENTS):
        logger.error("‚ùå –°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞/–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        return {"answer": "–°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.", "source_documents": [], "confidence": 0.0, "latency_sec": 0.0}

    start_time = time.time()
    
    retriever = CustomRetriever(vector_store=VECTOR_STORE, reranker=RERANKER)
    scored_documents: List[Tuple[Document, float]] = retriever.get_relevant_documents(question)
    
    latency = time.time() - start_time
    
    if not scored_documents:
        return {"answer": "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.", "source_documents": [], "confidence": 0.0, "latency_sec": latency}

    best_match_doc = scored_documents[0][0]
    top_score = scored_documents[0][1]

    normalized_query = question.strip().lower()
    priority_match: Optional[Document] = None
    max_priority = -1
    
    # –õ–æ–≥–∏–∫–∞ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
    for doc, score in scored_documents[:CONFIG["TOP_N_RERANKER"]]:
        doc_type = doc.metadata.get('chunk_type')
        doc_original_text = doc.metadata.get('original_text', '')
        
        if doc_type != 'T' and doc_original_text.strip().lower() == normalized_query and score > 0.8:
            priority = HEADER_PRIORITY.get(doc_type, 0)
            
            if priority > max_priority:
                max_priority = priority
                priority_match = doc

    if priority_match:
        best_match_doc = priority_match
        top_score = scored_documents[0][1] 
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    final_expanded_text = _expand_context(best_match_doc)
    
    
    if top_score < CONFIG['CONFIDENCE_THRESHOLD']:
        answer_text = (
            f"‚ö†Ô∏è –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –Ω–∏–∑–∫–∞ ({top_score:.2f})."
            "–õ—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ:\n" + best_match_doc.metadata.get('original_text', best_match_doc.page_content)
        )
    else:
        answer_type = best_match_doc.metadata.get('chunk_type')
        answer_label = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Ä–∞–∑–¥–µ–ª {answer_type})" if answer_type != 'T' else "–ù–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∞–±–∑–∞—Ü"
        
        answer_text = (
            f"‚úÖ {answer_label}:\n\n"
            f"{final_expanded_text}"
        )
    
    logger.info(f"‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞: {latency:.2f}s")
    
    return {
        "answer": answer_text,
        "source_documents": [best_match_doc],
        "confidence": top_score,
        "latency_sec": latency,
        "chunk_type": answer_type
    }
