import os
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import torch
import pickle
import json
import re
import time
from tqdm import tqdm
import random
import numpy as np
import warnings

from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.schema.retriever import BaseRetriever

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    BitsAndBytesConfig,
    pipeline,
    set_seed
)

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from qdrant_client.http.exceptions import UnexpectedResponse

from sentence_transformers import CrossEncoder

from rank_bm25 import BM25Okapi

from unstructured.partition.auto import partition
from unstructured.documents.elements import Element

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("QA_SYSTEM")

# –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π BitsAndBytes
warnings.filterwarnings("ignore", category=UserWarning)

# --- –ì–õ–û–ë–ê–õ–¨–ù–´–ï –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ü–ï–†–ï–ú–ï–ù–ù–´–ï ---
VECTOR_STORE: Optional[Qdrant] = None
EMBEDDINGS_MODEL: Optional[HuggingFaceEmbeddings] = None
RERANKER: Optional[CrossEncoder] = None
LLM_MODEL: Optional[HuggingFacePipeline] = None
BM25_INDEX: Optional[BM25Okapi] = None
BM25_CORPUS: List[str] = []

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´ ---
CONFIG = {
    "mode": "balanced",
    
    # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞
    "K_VEC": 20,          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏–∑ Qdrant
    "K_BM25": 20,         # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ BM25
    "ALPHA": 0.5,         # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç RRF (Reciprocal Rank Fusion)
    "TOP_N_RERANKER": 5,  # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
    "USE_HYDE": True,
    "SEED": 42,           # Random seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LLM
    "LLM_MODEL_NAME": "Qwen/Qwen2.5-7B-Instruct",
    "EMBEDDING_MODEL_NAME": "BAAI/bge-m3",
    "RERANKER_MODEL_NAME": "BAAI/bge-reranker-v2-m3",
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–ø–ª–∏—Ç—Ç–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    "CHUNK_SIZE": 1500,
    "CHUNK_OVERLAP": 200,
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞
    "BENCHMARK": {
        "N_QUESTIONS": 100,
        "types": {
            "factual": 0.25,
            "analytical": 0.25,
            "procedural": 0.20,
            "comparative": 0.15,
            "synthetic": 0.15
        },
        "difficulty_levels": {
            "easy": {
                "description": "–ü—Ä–æ—Å—Ç–æ–π —Ñ–∞–∫—Ç, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏",
                "answer_length_range": [20, 100],
                "chunks_required": 1
            },
            "medium": {
                "description": "–¢—Ä–µ–±—É–µ—Ç –æ–±–æ–±—â–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π",
                "answer_length_range": [100, 250],
                "chunks_required": 1
            },
            "hard": {
                "description": "–¢—Ä–µ–±—É–µ—Ç —Å–∏–Ω—Ç–µ–∑–∞ –∏–∑ 2-—Ö —Ä–∞–∑–Ω—ã—Ö —Å–µ–∫—Ü–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "answer_length_range": [250, 400],
                "chunks_required": 2
            },
            "expert": {
                "description": "–¢—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è, —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, –≤—ã–≤–æ–¥–∞ –Ω–µ—è–≤–Ω—ã—Ö —Å–≤—è–∑–µ–π",
                "answer_length_range": [400, 600],
                "chunks_required": 3
            }
        }
    }
}

# --- –£–¢–ò–õ–ò–¢–´ ---

def set_deterministic_mode(seed: int = CONFIG["SEED"]):
    """
    –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π seed –¥–ª—è –≤—Å–µ—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ (Python, NumPy, PyTorch, Transformers).
    
    –≠—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—É—é –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ RAG-—Å–∏—Å—Ç–µ–º—ã.
    
    Args:
        seed: –ß–∏—Å–ª–æ–≤–æ–π seed –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏.
    """
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    set_seed(seed)
    logger.info(f"‚öôÔ∏è  –°–∏—Å—Ç–µ–º–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∂–∏–º —Å SEED: {seed}")

def _initialize_llm():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Large Language Model (Qwen2.5-7B) —Å 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π.
    
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç HuggingFacePipeline –¥–ª—è
    –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π (do_sample=False) –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è
    –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π LLM_MODEL.
    """
    global LLM_MODEL
    logger.info(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ LLM: {CONFIG['LLM_MODEL_NAME']} (4-bit, {CONFIG['mode']})")
    
    try:
        # 4-bit quantization config (NF4)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        tokenizer = AutoTokenizer.from_pretrained(CONFIG["LLM_MODEL_NAME"])
        
        model = AutoModelForCausalLM.from_pretrained(
            CONFIG["LLM_MODEL_NAME"],
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
        )
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ pipeline –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        pipe = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=4096,
            do_sample=False,
            temperature=0.0,
            top_p=1.0,
            return_full_text=False
        )
        LLM_MODEL = HuggingFacePipeline(pipeline=pipe)
        logger.info("‚úÖ LLM –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞.")
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ LLM: {e}")
        LLM_MODEL = None

def _initialize_embeddings():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (BGE-M3) —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º HuggingFaceEmbeddings.
    
    –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π EMBEDDINGS_MODEL.
    """
    global EMBEDDINGS_MODEL
    logger.info(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Embedding: {CONFIG['EMBEDDING_MODEL_NAME']}")
    try:
        EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
            model_name=CONFIG["EMBEDDING_MODEL_NAME"],
            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
        )
        logger.info("‚úÖ Embedding –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Embedding: {e}")
        EMBEDDINGS_MODEL = None

def _initialize_reranker():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Cross-Encoder Reranker (BGE-Reranker-v2-m3).
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π RERANKER.
    """
    global RERANKER
    logger.info(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Reranker: {CONFIG['RERANKER_MODEL_NAME']}")
    try:
        RERANKER = CrossEncoder(
            CONFIG["RERANKER_MODEL_NAME"], 
            max_length=512,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        logger.info("‚úÖ Reranker –∑–∞–≥—Ä—É–∂–µ–Ω.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Reranker: {e}")
        RERANKER = None

def _initialize_vector_store():
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Qdrant –∫–ª–∏–µ–Ω—Ç –∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
    
    –°–æ–∑–¥–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é "transconsultant_kb", –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç.
    –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π VECTOR_STORE.
    """
    global VECTOR_STORE
    logger.info("‚è≥ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant...")
    try:
        client = QdrantClient(path="./qdrant_storage") 
        
        embedding_size = 1024 # BGE-M3 dimensionality
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        collection_name = "transconsultant_kb"
        try:
            client.get_collection(collection_name)
            logger.info(f"‚úÖ Qdrant: –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
        except UnexpectedResponse:
             client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(size=embedding_size, distance=Distance.COSINE)
            )
             logger.info(f"‚úÖ Qdrant: –ö–æ–ª–ª–µ–∫—Ü–∏—è '{collection_name}' —Å–æ–∑–¥–∞–Ω–∞.")
             
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–∞ LangChain VectorStore
        VECTOR_STORE = Qdrant(
            client=client, 
            collection_name=collection_name, 
            embeddings=EMBEDDINGS_MODEL
        )
        logger.info("‚úÖ Qdrant –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
    
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Qdrant: {e}")
        VECTOR_STORE = None

def initialize_qa_system(mode: str = CONFIG["mode"], force_reload: bool = False):
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã QA-—Å–∏—Å—Ç–µ–º—ã (LLM, Embeddings, Reranker, Qdrant).
    
    Args:
        mode: –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'balanced'). –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –¥–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏.
        force_reload: –§–ª–∞–≥ –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏ —Ö—Ä–∞–Ω–∏–ª–∏—â.
    """
    global LLM_MODEL, EMBEDDINGS_MODEL, RERANKER, VECTOR_STORE
    
    set_deterministic_mode()
    
    if force_reload or not (LLM_MODEL and EMBEDDINGS_MODEL and RERANKER and VECTOR_STORE):
        logger.info("\n" + "="*60)
        logger.info(f"üöÄ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø QA-–°–ò–°–¢–ï–ú–´ ({mode})")
        logger.info("="*60)
        
        if not EMBEDDINGS_MODEL:
            _initialize_embeddings()
        
        if not RERANKER:
            _initialize_reranker()
            
        if EMBEDDINGS_MODEL and not VECTOR_STORE:
            _initialize_vector_store()
            
        if not LLM_MODEL:
            _initialize_llm()
            
        logger.info("‚úÖ –í–°–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´ –ó–ê–ì–†–£–ñ–ï–ù–´.")
    else:
        logger.info("‚öôÔ∏è  –°–∏—Å—Ç–µ–º–∞ —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è.")

# --- –ü–ê–†–°–ò–ù–ì –ò –ò–ù–î–ï–ö–°–ê–¶–ò–Ø ---

def _parse_document_universal(file_path: Path) -> List[Document]:
    """
    –ü–∞—Ä—Å–∏—Ç DOCX, PDF, TXT —Å –ø–æ–º–æ—â—å—é 'unstructured', —Ä–∞–∑–¥–µ–ª—è–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏
    –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ LangChain Document.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Ç–∞–±–ª–∏—Ü—ã, –∑–∞–≥–æ–ª–æ–≤–∫–∏).
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
        
    Returns:
        –°–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ Document —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏.
    """
    logger.info(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ (Unstructured): {file_path.name}")
    
    try:
        elements: List[Element] = partition(filename=str(file_path))
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç
        full_text = "\n\n".join([str(el) for el in elements])
        
        logger.info(f"‚úÖ –§–∞–π–ª —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–æ–≤
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CONFIG["CHUNK_SIZE"],
            chunk_overlap=CONFIG["CHUNK_OVERLAP"],
            separators=["\n\n\n", "\n\n", "\n", " ", ""]
        )
        
        documents = text_splitter.create_documents(
            texts=[full_text],
            metadatas=[{"source": file_path.name, "doc_type": file_path.suffix.lstrip('.')}]
        )
        
        logger.info(f"üìö –†–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ {len(documents)} —á–∞–Ω–∫–æ–≤.")
        return documents
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {file_path.name}: {e}")
        return []

def update_bm25_index(documents: List[Document]):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç BM25 –∏–Ω–¥–µ–∫—Å –∏ –∫–æ—Ä–ø—É—Å, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–æ—Ä–ø—É—Å –¥–ª—è –±—É–¥—É—â–∏—Ö —Å–µ—Å—Å–∏–π.
    
    Args:
        documents: –°–ø–∏—Å–æ–∫ LangChain Document –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –∏–Ω–¥–µ–∫—Å.
    """
    global BM25_INDEX, BM25_CORPUS
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    texts = [doc.page_content for doc in documents]
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è BM25
    tokenized_corpus = [doc.split(" ") for doc in texts]
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞
    BM25_CORPUS.extend(texts)
    BM25_INDEX = BM25Okapi(tokenized_corpus)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞
    with open("bm25_corpus.pkl", "wb") as f:
        pickle.dump(BM25_CORPUS, f)
    
    logger.info(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å –æ–±–Ω–æ–≤–ª–µ–Ω: {len(BM25_CORPUS)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

def index_document(file_path: Path):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: –ø–∞—Ä—Å–∏–Ω–≥,
    –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î (Qdrant) –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞.
    
    Args:
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É.
    """
    global VECTOR_STORE
    
    documents = _parse_document_universal(file_path)
    
    if not documents:
        return

    # 1. –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î (Qdrant)
    if VECTOR_STORE:
        try:
            VECTOR_STORE.add_documents(documents)
            logger.info(f"‚úÖ Qdrant: –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤.")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ Qdrant: {e}")

    # 2. BM25 (–õ–µ–∫—Å–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫)
    update_bm25_index(documents)

def index_knowledge_base(folder_path: Path):
    """
    –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (.docx, .pdf, .txt) –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–∏.
    
    Args:
        folder_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.
    """
    logger.info(f"\n{'='*60}")
    logger.info(f"üìö –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô: {folder_path}")
    logger.info(f"{'='*60}\n")
    
    if not folder_path.exists():
        logger.error(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
        return
    
    supported_formats = ['.docx', '.pdf', '.txt']
    
    files_to_index = []
    for ext in supported_formats:
        files_to_index.extend(folder_path.glob(f"*{ext}"))
    
    if not files_to_index:
        logger.warning(f"‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ {folder_path}")
        return
    
    logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files_to_index)}")
    
    for file_path in tqdm(files_to_index, desc="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è"):
        index_document(file_path)
        
    logger.info(f"\n{'='*60}")
    logger.info(f"‚úÖ –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    logger.info(f"   –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {len(BM25_CORPUS)}")
    logger.info(f"{'='*60}\n")

# --- –†–ï–¢–†–ò–í–ï–† –ò RAG ---

class EnhancedHybridRetriever(BaseRetriever):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä, —Ä–µ–∞–ª–∏–∑—É—é—â–∏–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (Vector + BM25) —Å 
    –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Reciprocal Rank Fusion (RRF) –∏ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º Reranker.
    """
    
    def __init__(self, vector_store: Qdrant, reranker: CrossEncoder):
        self.vector_store = vector_store
        self.reranker = reranker
        self.k_vec = CONFIG["K_VEC"]
        self.k_bm25 = CONFIG["K_BM25"]
        self.alpha = CONFIG["ALPHA"]
        self.top_n = CONFIG["TOP_N_RERANKER"]

    def _get_relevant_documents(self, query: str, **kwargs) -> List[Document]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

        Args:
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å (HyDE).
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è).

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ Document (–ø–æ—Å–ª–µ RRF –∏ Reranking).
        """
        global BM25_INDEX, BM25_CORPUS
        
        # 1. Vector Search
        vec_results = self.vector_store.similarity_search_with_score(query, k=self.k_vec)
        
        # 2. BM25 Search
        bm25_results = []
        if BM25_INDEX and BM25_CORPUS:
            tokenized_query = query.split(" ")
            doc_scores = BM25_INDEX.get_scores(tokenized_query)
            
            # Get indices of top-K documents
            top_n_indices = np.argsort(doc_scores)[::-1][:self.k_bm25]
            
            for rank, idx in enumerate(top_n_indices):
                # Create Document for RRF fusion
                doc = Document(
                    page_content=BM25_CORPUS[idx],
                    metadata={"source": "bm25_hit", "rank": rank + 1}
                )
                bm25_results.append(doc)
        
        # 3. RRF (Reciprocal Rank Fusion)
        all_docs_map: Dict[str, Dict[str, Any]] = {}

        # RRF score calculation
        def calculate_rrf_score(rank, k=60):
            return 1 / (k + rank)

        # Process vector results
        for rank, (doc, score) in enumerate(vec_results):
            text_hash = hash(doc.page_content)
            
            if text_hash not in all_docs_map:
                all_docs_map[text_hash] = {"doc": doc, "rrf_score": 0.0}
            
            rrf_score = calculate_rrf_score(rank + 1)
            all_docs_map[text_hash]["rrf_score"] += rrf_score * self.alpha

        # Process BM25 results
        for rank, doc in enumerate(bm25_results):
            text_hash = hash(doc.page_content)
            
            if text_hash not in all_docs_map:
                all_docs_map[text_hash] = {"doc": doc, "rrf_score": 0.0}
            
            rrf_score = calculate_rrf_score(rank + 1)
            all_docs_map[text_hash]["rrf_score"] += rrf_score * (1.0 - self.alpha)

        # Sort by RRF Score
        final_candidates = sorted(
            all_docs_map.values(),
            key=lambda x: x["rrf_score"],
            reverse=True
        )
        
        # Extract documents for reranking
        rerank_input = [item["doc"] for item in final_candidates]
        
        # 4. Reranking
        if self.reranker and len(rerank_input) > 0:
            pairs = [[query, doc.page_content] for doc in rerank_input]
            scores = self.reranker.predict(pairs)
            
            # Merge documents and scores
            scored_documents = sorted(
                zip(rerank_input, scores),
                key=lambda x: x[1],
                reverse=True
            )
            
            # 5. Return top-N documents
            final_docs = [doc for doc, score in scored_documents[:self.top_n]]
        else:
            # Fallback to top-N based on RRF
            final_docs = [item["doc"] for item in final_candidates[:self.top_n]]

        return final_docs

def _apply_hyde(question: str) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç (HyDE) –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
    –∏ –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞.

    Args:
        question: –ò—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

    Returns:
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å, –æ–±—ä–µ–¥–∏–Ω—è—é—â–∏–π –∏—Å—Ö–æ–¥–Ω—ã–π –≤–æ–ø—Ä–æ—Å –∏ –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç.
    """
    global LLM_MODEL
    if not LLM_MODEL:
        logger.warning("‚ö†Ô∏è LLM –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è HyDE.")
        return question

    # HyDE prompt
    hyde_prompt = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –¥–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–π, –Ω–æ –ø–æ–ª–Ω—ã–π –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç "
        "–Ω–∞ –≤–æ–ø—Ä–æ—Å, —á—Ç–æ–±—ã —É–ª—É—á—à–∏—Ç—å –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û—Ç–≤–µ—á–∞–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–≤–æ–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –∑–Ω–∞–Ω–∏–π. "
        f"–í–æ–ø—Ä–æ—Å: {question}"
    )

    try:
        response = LLM_MODEL.pipeline(
            hyde_prompt,
            max_new_tokens=150,
            do_sample=False,
            temperature=0.0,
            return_full_text=False
        )
        
        hypothetical_answer = response[0]['generated_text'].strip()
        expanded_query = f"{question} {hypothetical_answer}"
        
        logger.debug(f"üìù HyDE: '{hypothetical_answer[:80]}...'")
        return expanded_query
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ HyDE: {e}")
        return question

def answer_question(question: str) -> Dict[str, Any]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG-—Ü–µ–ø–æ—á–∫–∏.
    
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç HyDE (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω), –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç
    –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é LLM —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π Chain-of-Thought (CoT).

    Args:
        question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º, –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ –∑–∞–¥–µ—Ä–∂–∫–æ–π:
            - answer (str): –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç LLM.
            - source_documents (List[Document]): –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
            - latency_sec (float): –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö.
    """
    global LLM_MODEL, VECTOR_STORE, RERANKER
    
    if not (LLM_MODEL and VECTOR_STORE and RERANKER):
        logger.error("‚ùå –°–∏—Å—Ç–µ–º–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        return {"answer": "–°–∏—Å—Ç–µ–º–∞ QA –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.", "source_documents": []}
    
    start_time = time.time()
    
    # 1. HyDE application
    expanded_query = _apply_hyde(question) if CONFIG["USE_HYDE"] else question

    # 2. Retriever setup
    retriever = EnhancedHybridRetriever(VECTOR_STORE, RERANKER)
    
    # 3. RetrievalQA Chain setup
    QA_PROMPT = PromptTemplate(
        template=(
            "–¢—ã ‚Äî –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω—ã–π QA-–±–æ—Ç. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. "
            "–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ–¥–∏ –ê–ù–ê–õ–ò–ó (—Ü–µ–ø–æ—á–∫—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π) —Ç–æ–≥–æ, –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å. "
            "–ó–∞—Ç–µ–º –¥–∞–π –ö–†–ê–¢–ö–ò–ô, –Ω–æ –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç—Ä–æ–≥–∏–π —Ñ–æ—Ä–º–∞—Ç.\n\n"
            "–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–ê–í–ò–õ–ê: "
            "1. –ù–µ –≥–æ–≤–æ—Ä–∏, —á—Ç–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –º–æ–∂–Ω–æ —Å–æ—Å—Ç–∞–≤–∏—Ç—å. "
            "2. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. "
            "3. –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ù–ï —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏ '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞.'\n\n"
            "–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n"
            "–í–û–ü–†–û–°: {question}\n\n"
            "–ê–ù–ê–õ–ò–ó: " # Chain-of-Thought instruction
        ),
        input_variables=["context", "question"]
    )
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=LLM_MODEL,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": QA_PROMPT},
        return_source_documents=True
    )

    result = qa_chain({"query": expanded_query})
    
    end_time = time.time()
    latency = end_time - start_time
    
    # 4. Final answer extraction
    raw_answer = result['result'].strip()
    match = re.search(r'–û–¢–í–ï–¢:\s*(.*)', raw_answer, re.DOTALL | re.IGNORECASE) 
    
    if match:
        final_answer = match.group(1).strip()
    else:
        # Fallback: use entire text if "–û–¢–í–ï–¢:" label is missing
        final_answer = raw_answer
    
    logger.info(f"‚è±Ô∏è  –ó–∞–¥–µ—Ä–∂–∫–∞: {latency:.2f}s")
    
    return {
        "answer": final_answer,
        "source_documents": result["source_documents"],
        "latency_sec": latency
    }


# --- –§–£–ù–ö–¶–ò–ò –ë–ï–ù–ß–ú–ê–†–ö–ê ---

def generate_diverse_benchmark(corpus: List[str], N: int = 100) -> List[Dict[str, Any]]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–∞–±–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ (–±–µ–Ω—á–º–∞—Ä–∫), —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –ø–æ —Ç–∏–ø—É –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏,
    –∏—Å–ø–æ–ª—å–∑—É—è LLM –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞.
    
    Args:
        corpus: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤, –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ç–æ—Ä—ã—Ö –±—É–¥—É—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤–æ–ø—Ä–æ—Å—ã.
        N: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π, –≥–¥–µ –∫–∞–∂–¥—ã–π —Å–ª–æ–≤–∞—Ä—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤–æ–ø—Ä–æ—Å
        —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ ('question', 'type', 'difficulty', 'min_chunks_required').
    """
    global LLM_MODEL
    if not LLM_MODEL or not corpus:
        logger.error("‚ùå LLM –∏–ª–∏ –ö–æ—Ä–ø—É—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞.")
        return []

    type_weights = CONFIG["BENCHMARK"]["types"]
    difficulty_map = CONFIG["BENCHMARK"]["difficulty_levels"]
    
    # –í—ã–±–æ—Ä —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—á–∞—Å—Ç—å –∫–æ—Ä–ø—É—Å–∞) –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    num_docs_for_gen = min(10, len(corpus))
    selected_docs = random.sample(corpus, num_docs_for_gen)
    context_for_generation = "\n\n---\n\n".join(selected_docs)

    benchmark_questions = []

    for q_type, weight in type_weights.items():
        num_q_for_type = round(N * weight)
        if num_q_for_type == 0: continue

        # –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞
        difficulty_name = random.choice(list(difficulty_map.keys()))
        difficulty_config = difficulty_map[difficulty_name]
        
        generation_prompt = (
            f"–ò—Å–ø–æ–ª—å–∑—É—è —Å–ª–µ–¥—É—é—â–∏–π –ö–û–ù–¢–ï–ö–°–¢, —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π {num_q_for_type} –≤–æ–ø—Ä–æ—Å–æ–≤ "
            f"—Ç–∏–ø–∞ '{q_type}' –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ '{difficulty_name}'. "
            f"–¢—Ä–µ–±—É–µ–º–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {difficulty_config['description']}. "
            f"–í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –∏ —Ç—Ä–µ–±–æ–≤–∞—Ç—å {difficulty_config['chunks_required']} —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞.\n\n"
            f"–ö–û–ù–¢–ï–ö–°–¢: {context_for_generation}\n\n"
            "–ì–ï–ù–ï–†–ò–†–£–ô –¢–û–õ–¨–ö–û –°–ü–ò–°–û–ö –í–û–ü–†–û–°–û–í (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É) –ë–ï–ó –ù–£–ú–ï–†–ê–¶–ò–ò –ò –õ–Æ–ë–û–ì–û –î–†–£–ì–û–ì–û –¢–ï–ö–°–¢–ê."
        )

        try:
            # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤
            response = LLM_MODEL.pipeline(
                generation_prompt,
                max_new_tokens=4096,
                do_sample=False, 
                temperature=0.0,
                return_full_text=False
            )
            
            generated_text = response[0]['generated_text'].strip()
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–æ–≤
            questions = [
                q.strip() for q in generated_text.split('\n') 
                if q.strip() and len(q.strip()) > 10
            ]
            
            for q in questions:
                benchmark_questions.append({
                    "question": q,
                    "type": q_type,
                    "difficulty": difficulty_name,
                    "min_chunks_required": difficulty_config['chunks_required']
                })

            logger.info(f"   ‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ '{q_type}'")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ —Ç–∏–ø–∞ {q_type}: {e}")
            continue

    logger.info(f"üìä –í—Å–µ–≥–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(benchmark_questions)}")
    return benchmark_questions

def run_benchmark(benchmark_questions: List[Dict[str, Any]], save_path: str = "benchmark_results.json") -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –æ—Ü–µ–Ω–∫—É QA-—Å–∏—Å—Ç–µ–º—ã –Ω–∞ –Ω–∞–±–æ—Ä–µ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON.

    Args:
        benchmark_questions: –°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–µ–π generate_diverse_benchmark.
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏.

    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –æ–±—â–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
    """
    logger.info(f"\n{'='*60}")
    logger.info("‚ö°Ô∏è –ó–ê–ü–£–°–ö –ë–ï–ù–ß–ú–ê–†–ö–ê (–î–ï–ì–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–´–ô –¢–ï–°–¢)")
    logger.info("="*60 + "\n")
    
    results = []
    successful_answers = 0
    total_latency = 0.0
    
    for item in tqdm(benchmark_questions, desc="–û—Ü–µ–Ω–∫–∞ RAG"):
        question = item["question"]
        
        try:
            result = answer_question(question)
            
            results.append({
                "question": question,
                "type": item["type"],
                "difficulty": item["difficulty"],
                "answer": result["answer"],
                "latency_sec": result["latency_sec"],
                "sources_found": [doc.metadata.get("source", "N/A") for doc in result["source_documents"]]
            })
            
            total_latency += result["latency_sec"]
            if result["answer"] not in ["–°–∏—Å—Ç–µ–º–∞ QA –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.", "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞."]:
                successful_answers += 1
                
        except Exception as e:
            results.append({
                "question": question,
                "type": item["type"],
                "difficulty": item["difficulty"],
                "answer": f"ERROR: {e}",
                "latency_sec": 0.0,
                "sources_found": []
            })
            
    # –ü–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
    total_questions = len(benchmark_questions)
    final_metrics = {
        "total_questions": total_questions,
        "successful_answers": successful_answers,
        "success_rate": successful_answers / total_questions if total_questions > 0 else 0.0,
        "avg_latency_sec": total_latency / successful_answers if successful_answers > 0 else 0.0,
    }
    
    logger.info(f"\n{'='*60}")
    logger.info("‚úÖ –ë–ï–ù–ß–ú–ê–†–ö –ó–ê–í–ï–†–®–ï–ù")
    logger.info(f"üìä –£—Å–ø–µ—à–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {final_metrics['successful_answers']}/{final_metrics['total_questions']} ({(final_metrics['success_rate']*100):.2f}%)")
    logger.info(f"‚è±Ô∏è  –°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞: {final_metrics['avg_latency_sec']:.2f}s")
    logger.info(f"{'='*60}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    output = {
        "results": results,
        "metrics": final_metrics
    }
    
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {save_path}")
    
    return output


# --- –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ---

if __name__ == '__main__':
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
    initialize_qa_system()
    
    # 2. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (DOCX, PDF, TXT)
    # –ü—É—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–∞—Ç–∞–ª–æ–≥–∞ TransConsultant/backend/qa/
    docs_folder_path = Path("../../docs")
    
    if docs_folder_path.exists():
        index_knowledge_base(docs_folder_path)
    else:
        logger.warning(f"‚ö†Ô∏è –ü–∞–ø–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {docs_folder_path.resolve()}")
        logger.info("üí° –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É 'docs' –Ω–∞ –∫–æ—Ä–Ω–µ–≤–æ–º —É—Ä–æ–≤–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ TransConsultant")
    
    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞
    if BM25_CORPUS and len(BM25_CORPUS) > 5:
        logger.info("\n" + "="*60)
        logger.info("üéØ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ë–ï–ù–ß–ú–ê–†–ö–ê")
        logger.info("="*60 + "\n")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 100 –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä–ø—É—Å–∞
        benchmark = generate_diverse_benchmark(BM25_CORPUS, N=CONFIG["BENCHMARK"]["N_QUESTIONS"])
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–æ–≤
        with open("benchmark_questions.json", "w", encoding="utf-8") as f:
            json.dump(benchmark, f, indent=2, ensure_ascii=False)
        
        logger.info("‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: benchmark_questions.json")
        
        # 4. –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏
        if benchmark:
            run_benchmark(benchmark, save_path="final_benchmark_metrics.json")
    else:
        logger.warning("‚ö†Ô∏è –ö–æ—Ä–ø—É—Å —Å–ª–∏—à–∫–æ–º –º–∞–ª –∏–ª–∏ –ø—É—Å—Ç. –ë–µ–Ω—á–º–∞—Ä–∫ –Ω–µ –∑–∞–ø—É—â–µ–Ω.")