import logging
import time
import numpy as np
import torch
from tqdm import tqdm
from docx import Document as DocxDocument
from sentence_transformers import CrossEncoder
from rank_bm25 import BM25Okapi
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain_core.documents import Document
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams
from django.conf import settings

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
VECTOR_STORE = None
EMBEDDINGS_MODEL = None
RERANKER = None

BM25_INDEX = None
BM25_CORPUS = []
FULL_INDEXED_DOCUMENTS = []

HEADER_PRIORITY = {'H0': 6.0, 'H1': 5.0, 'H2': 4.0, 'H3': 3.0, 'H4': 2.0, 'T': 1.0, 'L': 1.0}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ QA
def initialize_embeddings():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π (embeddings)."""
    global EMBEDDINGS_MODEL

    embedding_model_name = settings.QA_CONFIG.get("EMBEDDING_MODEL_NAME")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Embedding: {embedding_model_name}")
    try:
        EMBEDDINGS_MODEL = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs={'device': device}
        )
        logger.info(f"‚úÖ Embedding –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞: {device}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Embedding: {e}")
        EMBEDDINGS_MODEL = None

def initialize_reranker():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è (reranker)."""
    global RERANKER

    reranker_model_name = settings.QA_CONFIG.get("RERANKER_MODEL_NAME", "BAAI/bge-reranker-v2-m3")
    reranker_max_length = settings.QA_CONFIG.get("RERANKER_MAX_LENGTH", 512)
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Reranker: {reranker_model_name}")
    try:
        RERANKER = CrossEncoder(
            reranker_model_name,
            max_length=reranker_max_length,
            device=device
        )
        logger.info(f"‚úÖ Reranker –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞: {device}")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Reranker: {e}")
        RERANKER = None

def initialize_vector_store():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ Qdrant."""
    global VECTOR_STORE, EMBEDDINGS_MODEL
    logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Qdrant...")
    if not EMBEDDINGS_MODEL:
        logger.error("‚ùå EMBEDDINGS_MODEL –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
        return
    try:
        qdrant_path = settings.QA_CONFIG.get("QDRANT_PATH")
        qdrant_name = settings.QA_CONFIG.get("QDRANT_NAME")
        client = QdrantClient(path=qdrant_path)

        VECTOR_STORE = Qdrant(
            client=client, 
            collection_name=qdrant_name, 
            embeddings=EMBEDDINGS_MODEL
        )
        logger.info("‚úÖ Qdrant –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Qdrant: {e}")
        VECTOR_STORE = None

def initialize_qa_system():
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ QA-—Å–∏—Å—Ç–µ–º—ã."""
    global EMBEDDINGS_MODEL, RERANKER, VECTOR_STORE
    
    if not (EMBEDDINGS_MODEL and RERANKER and VECTOR_STORE):
        logger.info(f"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã QA")
        if not EMBEDDINGS_MODEL: initialize_embeddings()
        if not RERANKER: initialize_reranker()
        if EMBEDDINGS_MODEL and not VECTOR_STORE: initialize_vector_store()
        logger.info("‚úÖ QA —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")

# –ü–∞—Ä—Å–∏–Ω–≥ DOCX
def is_bold_paragraph(p):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –≤ DOCX-–¥–æ–∫—É–º–µ–Ω—Ç–µ –∂–∏—Ä–Ω—ã–º."""
    text = p.text.strip()
    if not text:
        return False

    for run in p.runs:
        if run.text.strip(): 
            return run.bold or ('strong' in run.element.xml) 
    return False

def equals_indent(indent_emu, target_cm, eps_cm=0.01):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ—Ç—Å—Ç—É–ø –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ —Å —Ü–µ–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –≤ —Å–∞–Ω—Ç–∏–º–µ—Ç—Ä–∞—Ö."""
    return abs(indent_emu.cm - target_cm) < eps_cm

def get_chunk_type(paragraph):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Ç–∏–ø –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (–∑–∞–≥–æ–ª–æ–≤–æ–∫, —Ç–µ–∫—Å—Ç, —Å–ø–∏—Å–æ–∫)."""
    style_name = paragraph.style.name.lower()
    text = paragraph.text.strip()
    if not text: return None
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å—Ç–∏–ª–∏ (H0, H1, H2)
    if style_name.startswith('heading 1') or '–∑–∞–≥–æ–ª–æ–≤–æ–∫ 1' in style_name:
        return 'H0'
    if style_name.startswith('heading 2') or '–∑–∞–≥–æ–ª–æ–≤–æ–∫ 2' in style_name:
        return 'H1'
    if style_name.startswith('heading 3') or '–∑–∞–≥–æ–ª–æ–≤–æ–∫ 3' in style_name:
        return 'H2'
        
    # –õ–æ–≥–∏–∫–∞ –¥–ª—è H3 –∏ H4 ("–û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç" + –ñ–∏—Ä–Ω–æ—Å—Ç—å + –û—Ç—Å—Ç—É–ø)
    is_normal_style = 'normal' in style_name or '–æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç' in style_name
    
    if is_normal_style and is_bold_paragraph(paragraph):

        first_line_indent_style = paragraph.style.paragraph_format.first_line_indent
        first_line_indent = paragraph.paragraph_format.first_line_indent

        if first_line_indent is not None and equals_indent(first_line_indent, 1.25):
            return 'H4'
        
        if first_line_indent is None and equals_indent(first_line_indent_style, 0.75):
            return 'H3'
        
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ø–∏—Å–∫–∏ L
    if 'list' in style_name or 'bullet' in style_name or 'number' in style_name:
        return 'L'

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ü–∏—Ç–∞—Ç—ã, –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç T
    if 'quote' in style_name or 'normal' in style_name:
        return None
        
    return None

def parse_document_universal(file_path):
    """–ü–∞—Ä—Å–∏—Ç DOCX-—Ñ–∞–π–ª, —Ä–∞–∑–±–∏–≤–∞—è –µ–≥–æ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ç–µ–∫—Å—Ç)."""
    logger.info(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {file_path.name}")
    
    if file_path.suffix.lstrip('.') != 'docx':
        logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω —Ñ–∞–π–ª: {file_path.name}. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ DOCX.")
        return []
         
    documents = []
    paragraphs_data = [p for p in DocxDocument(file_path).paragraphs if p.text.strip()]
        
    i = 0
    paragraphs_data_count = len(paragraphs_data)
    
    current_header_h0: str = "" 
    current_header_h1: str = "" 
    current_header_h2: str = ""
    current_header_h3: str = "" 
    current_header_h4: str = ""
    
    while i < paragraphs_data_count:
        
        current_paragraph_obj = paragraphs_data[i]
        current_paragraph = current_paragraph_obj.text.strip()
        header_level = get_chunk_type(current_paragraph_obj)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏, –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –∞–±–∑–∞—Ü ‚Äî –∑–∞–≥–æ–ª–æ–≤–æ–∫
        if header_level and header_level != 'L':
            
            # –õ–æ–≥–∏–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ H0-H4)
            if header_level == 'H0':
                current_header_h0 = current_paragraph
                current_header_h1 = current_header_h2 = current_header_h3 = current_header_h4 = ""
            elif header_level == 'H1':
                current_header_h1 = current_paragraph
                current_header_h2 = current_header_h3 = current_header_h4 = ""
            elif header_level == 'H2':
                current_header_h2 = current_paragraph
                current_header_h3 = current_header_h4 = ""
            elif header_level == 'H3':
                current_header_h3 = current_paragraph
                current_header_h4 = ""
            elif header_level == 'H4':
                current_header_h4 = current_paragraph
            
            header_context_parts = [h for h in [current_header_h0, current_header_h1, current_header_h2, current_header_h3, current_header_h4] if h]
            header_context = " | ".join(header_context_parts)
            header_chunk_content = f"[{header_context}] {current_paragraph}"
            
            documents.append(
                Document(
                    page_content=header_chunk_content,
                    metadata={
                        "source": file_path.name,
                        "chunk_type": header_level, 
                        "h0_header": current_header_h0,
                        "h1_header": current_header_h1,
                        "h2_header": current_header_h2,
                        "h3_header": current_header_h3,
                        "h4_header": current_header_h4,
                        "start_index": i,            
                        "end_index": i,              
                        "original_text": current_paragraph
                    }
                )
            )
            i += 1 
            continue
            
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ T –∏ L
        current_chunk_raw_texts = [current_paragraph]
        start_paragraph_index = i
        j = i
        final_chunk_type = header_level if header_level is not None else 'T'
        
        ends_with_colon = current_paragraph.rstrip().endswith(':')
        should_aggregate = False

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ç–µ–∫—Å—Ç ":" + –°–ø–∏—Å–æ–∫ "L"
        if final_chunk_type == 'T' and ends_with_colon and (j + 1 < paragraphs_data_count):
            next_paragraph_type = get_chunk_type(paragraphs_data[j + 1])
            if next_paragraph_type == 'L':
                should_aggregate = True
        
        # –¶–∏–∫–ª –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
        if should_aggregate:
            final_chunk_type = 'T' 

            while j + 1 < paragraphs_data_count:
                next_paragraph_obj = paragraphs_data[j + 1]
                next_paragraph_text = next_paragraph_obj.text.strip()
                
                if not next_paragraph_text:
                    j += 1
                    continue

                next_header_level = get_chunk_type(next_paragraph_obj) 

                if next_header_level != 'L':
                    break
                    
                current_chunk_raw_texts.append(next_paragraph_text)
                j += 1
        
        current_chunk_raw = "\n".join(current_chunk_raw_texts)
        
        header_context_parts = [h for h in [current_header_h0, current_header_h1, current_header_h2, current_header_h3, current_header_h4] if h]
        header_context = " | ".join(header_context_parts)
        final_chunk_content = f"[{header_context}] {current_chunk_raw}"

        documents.append(
            Document(
                page_content=final_chunk_content,
                metadata={
                    "source": file_path.name,
                    "chunk_type": final_chunk_type, 
                    "h0_header": current_header_h0,
                    "h1_header": current_header_h1,
                    "h2_header": current_header_h2,
                    "h3_header": current_header_h3,
                    "h4_header": current_header_h4,
                    "start_index": start_paragraph_index,
                    "end_index": j,
                    "original_text": current_chunk_raw
                }
            )
        )
        
        # –ü–µ—Ä–µ–¥–≤–∏–≥–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∏–Ω–¥–µ–∫—Å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –∞–±–∑–∞—Ü –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö
        i = j + 1 

    logger.info(f"üìö –†–∞–∑–¥–µ–ª–µ–Ω–æ –Ω–∞ {len(documents)} —á–∞–Ω–∫–æ–≤.")
    return documents


# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞
def update_bm25_index(documents):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç BM25-–∏–Ω–¥–µ–∫—Å –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    global BM25_INDEX, BM25_CORPUS, FULL_INDEXED_DOCUMENTS

    texts = [doc.page_content for doc in documents]

    if not FULL_INDEXED_DOCUMENTS or len(FULL_INDEXED_DOCUMENTS) == 0:
         BM25_CORPUS = []
         FULL_INDEXED_DOCUMENTS = []
    
    BM25_CORPUS.extend(texts)
    FULL_INDEXED_DOCUMENTS.extend(documents)
    tokenized_corpus = [doc.split(" ") for doc in BM25_CORPUS]
    BM25_INDEX = BM25Okapi(tokenized_corpus)
    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å—ã (Qdrant, BM25) –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (FULL_INDEXED_DOCUMENTS) –æ–±–Ω–æ–≤–ª–µ–Ω—ã.")

def chunk_list(data, size):
    """–†–∞–∑–¥–µ–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–∞ –±–∞—Ç—á–∏ –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞."""
    for i in range(0, len(data), size):
        yield data[i:i + size]

def index_document(file_path):
    """–ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç, –¥–æ–±–∞–≤–ª—è—è –µ–≥–æ —á–∞–Ω–∫–∏ –≤ Qdrant –∏ BM25."""
    global VECTOR_STORE

    documents = parse_document_universal(file_path)
    BATCH_SIZE = settings.QA_CONFIG.get("QDRANT_BATCH_SIZE", 32)

    if not documents: return
    if VECTOR_STORE:
        try:
            logger.info(f"üîÑ Qdrant: –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {len(documents)} —á–∞–Ω–∫–æ–≤ (–±–∞—Ç—á: {BATCH_SIZE}).")
            chunk_batches = list(chunk_list(documents, BATCH_SIZE))
            
            for batch in tqdm(chunk_batches, desc=f"Qdrant: {file_path.name}", unit="batch"):
                VECTOR_STORE.add_documents(batch) 
            
            logger.info(f"‚úÖ Qdrant: –î–æ–±–∞–≤–ª–µ–Ω–æ {len(documents)} —á–∞–Ω–∫–æ–≤.")
        except Exception as e:
             logger.error(f"‚ùå –û—à–∏–±–∫–∞ Qdrant –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")
    update_bm25_index(documents)

def index_knowledge_base(folder_path):
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–π –ø–∞–ø–∫–∏."""
    logger.info(f"üìö –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
    if not folder_path.exists():
        logger.error(f"‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {folder_path}")
        return
    
    supported_formats = ['.docx'] 
    files_to_index = []

    for ext in supported_formats:
        files_to_index.extend([f for f in folder_path.glob(f"*{ext}") if not f.name.startswith('~$')])
    
    if not files_to_index:
        logger.warning(f"‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ DOCX –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ {folder_path}")
        return
    
    if VECTOR_STORE:
        VECTOR_STORE.client.recreate_collection(
            collection_name=VECTOR_STORE.collection_name,
            vectors_config=VectorParams(size=1024, distance=Distance.COSINE)
        )
        logger.info(f"üóëÔ∏è Qdrant: –ö–æ–ª–ª–µ–∫—Ü–∏—è '{VECTOR_STORE.collection_name}' –æ—á–∏—â–µ–Ω–∞.")
        
    global BM25_CORPUS, BM25_INDEX, FULL_INDEXED_DOCUMENTS
    BM25_CORPUS = []
    BM25_INDEX = None
    FULL_INDEXED_DOCUMENTS = []

    for file_path in tqdm(files_to_index, desc="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è"):
        index_document(file_path)

    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    logger.info(f"    –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ: {len(FULL_INDEXED_DOCUMENTS)}")

# –†–µ—Ç–∏–≤–µ—Ä
class CustomRetriever:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫, BM25 –∏ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ."""
    def __init__(self, vector_store, reranker):
        self.vector_store = vector_store
        self.reranker = reranker

    def get_relevant_documents(self, query):
        """–ù–∞—Ö–æ–¥–∏—Ç –∏ —Ä–∞–Ω–∂–∏—Ä—É–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞."""
        global BM25_INDEX, BM25_CORPUS
        
        vec_results = self.vector_store.similarity_search_with_score(query, k=settings.QA_CONFIG.get("K_VEC", 40))
        bm25_results = []

        if BM25_INDEX and BM25_CORPUS:
            tokenized_query = query.split(" ")
            doc_scores = BM25_INDEX.get_scores(tokenized_query)
            top_n_indices = np.argsort(doc_scores)[::-1][:settings.QA_CONFIG.get("K_BM25", 40)]
            
            for idx in top_n_indices:
                doc = FULL_INDEXED_DOCUMENTS[idx]
                bm25_results.append(doc)

        all_docs_map = {}
        K_RRF = 60 
        def calculate_rrf_score(rank, k=K_RRF): return 1 / (k + rank)

        for rank, (doc, _) in enumerate(vec_results):
            text_hash = hash(doc.page_content)
            if text_hash not in all_docs_map: all_docs_map[text_hash] = {"doc": doc, "rrf_score": 0.0}
            all_docs_map[text_hash]["rrf_score"] += calculate_rrf_score(rank + 1) * settings.QA_CONFIG.get("ALPHA", 0.5)

        for rank, doc in enumerate(bm25_results):
            text_hash = hash(doc.page_content)
            if text_hash not in all_docs_map: all_docs_map[text_hash] = {"doc": doc, "rrf_score": 0.0}
            all_docs_map[text_hash]["rrf_score"] += calculate_rrf_score(rank + 1) * (1.0 - settings.QA_CONFIG.get("ALPHA", 0.5))

        final_candidates = sorted(all_docs_map.values(), key=lambda x: x["rrf_score"], reverse=True)
        rerank_input = [item["doc"] for item in final_candidates[:50]]
        
        if self.reranker and len(rerank_input) > 0:
            pairs = [[query, doc.page_content] for doc in rerank_input]
            scores = self.reranker.predict(pairs)
            probabilities = 1 / (1 + np.exp(-scores))
            
            scored_documents = sorted(
                zip(rerank_input, probabilities),
                key=lambda x: x[1],
                reverse=True
            )
            return scored_documents[:settings.QA_CONFIG.get("TOP_N_RERANKER", 5)]
        
        return []

# –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ—Ç–≤–µ—Ç
def expand_context(best_match_doc):
    """–†–∞—Å—à–∏—Ä—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞, —Å–æ–±–∏—Ä–∞—è –ø–æ—Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç."""
    global FULL_INDEXED_DOCUMENTS, HEADER_PRIORITY
    
    chunk_type = best_match_doc.metadata.get('chunk_type')
    original_text_target = best_match_doc.metadata.get('original_text', best_match_doc.page_content)
    
    # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–∫—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
    if chunk_type == 'T':
        return original_text_target
        
    target_level = chunk_type 
    target_priority = HEADER_PRIORITY.get(target_level, 0)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    start_index_in_full = best_match_doc.metadata.get('start_index', -1) 
    
    if start_index_in_full == -1:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: –ù–µ –Ω–∞–π–¥–µ–Ω start_index –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞. –í–æ–∑–≤—Ä–∞—Ç —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞.")
        return original_text_target
        
    expanded_text = []
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∞–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    expanded_text.append(f"<{target_level}> {original_text_target} </{target_level}>")
    
    # –ù–∞—á–∏–Ω–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
    for idx in range(start_index_in_full + 1, len(FULL_INDEXED_DOCUMENTS)):
        current_doc = FULL_INDEXED_DOCUMENTS[idx]
        current_type = current_doc.metadata.get('chunk_type')
        current_priority = HEADER_PRIORITY.get(current_type, 0)
        current_original_text = current_doc.metadata.get('original_text', '')
        
        # –£—Å–ª–æ–≤–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è
        if current_type != 'T' and current_priority >= target_priority:
            break
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç (–¢–µ–∫—Å—Ç –∏–ª–∏ –ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–æ–ª–µ–µ –Ω–∏–∑–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è)
        if current_type == 'T':
            expanded_text.append(current_original_text)
        else:
            expanded_text.append(f"<{current_type}> {current_original_text} </{current_type}>")
            
    logger.info(f"‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–æ–±—Ä–∞–Ω–æ {len(expanded_text)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤.")
    return "\n\n".join(expanded_text)

def answer_question(question):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –≤—Å—é —Ü–µ–ø–æ—á–∫—É RAG."""
    global VECTOR_STORE, RERANKER, FULL_INDEXED_DOCUMENTS, HEADER_PRIORITY
    
    if not (VECTOR_STORE and RERANKER and FULL_INDEXED_DOCUMENTS):
        logger.error("‚ùå –°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞/–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.")
        return {"answer": "–°–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞.", "source_documents": [], "confidence": 0.0, "latency_sec": 0.0}

    start_time = time.time()
    
    retriever = CustomRetriever(vector_store=VECTOR_STORE, reranker=RERANKER)
    scored_documents = retriever.get_relevant_documents(question)
    
    latency = time.time() - start_time
    
    if not scored_documents:
        return {"answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.", "source_documents": [], "confidence": 0.0, "latency_sec": latency}

    best_match_doc = scored_documents[0][0]
    top_score = scored_documents[0][1]

    normalized_query = question.strip().lower()
    priority_match = None
    max_priority = -1
    
    # –õ–æ–≥–∏–∫–∞ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–æ–≥–æ –≤—ã–±–æ—Ä–∞
    for doc, score in scored_documents[:settings.QA_CONFIG.get("TOP_N_RERANKER", 5)]:
        doc_type = doc.metadata.get('chunk_type')
        doc_original_text = doc.metadata.get('original_text', '')
        
        if doc_type != 'T' and doc_original_text.strip().lower() == normalized_query and score > 0.8:
            priority = HEADER_PRIORITY.get(doc_type, 0)
            
            if priority > max_priority:
                max_priority = priority
                priority_match = doc

    if priority_match:
        best_match_doc = priority_match
        top_score = scored_documents[0][1] 
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    final_expanded_text = expand_context(best_match_doc)
    
    
    answer_text = f"{final_expanded_text}"
    answer_type = best_match_doc.metadata.get('chunk_type')
    
    logger.info(f"‚è±Ô∏è –û—Ç–≤–µ—Ç –¥–∞–Ω –∑–∞: {latency:.2f}s")
    
    if top_score < settings.QA_CONFIG.get("CONFIDENCE_THRESHOLD", 0.55):
        logger.warning(f"‚ö†Ô∏è –ù–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è: {top_score:.2f}")
        return {
            "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.",
            "source_documents": [best_match_doc],
            "confidence": top_score,
            "latency_sec": latency,
            "chunk_type": answer_type
        }

    return {
        "answer": answer_text,
        "source_documents": [best_match_doc],
        "confidence": top_score,
        "latency_sec": latency,
        "chunk_type": answer_type
    }
